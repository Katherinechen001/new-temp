[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Housing Prices",
    "section": "",
    "text": "1 Introduction\nIntroduction The US housing market is affected by multiple factors that result from property physical characteristics, the location related and structural feature. For would-be buyers and sellers, real estate professionals, policy-makers and investors who are seeking data-driven insights, knowing what is behind the price of a house is important. In this experiment, I analyzed what the most important factors are behind the housing price in the Us with a Kaggle dataset (545 observations and 13 features) with what I would call intuitive variables such as area, bedrooms, bathrooms, whether the house has any amenities (e.g., guestroom, basement, air condition) and furnishing status. To perform this analysis I use a set of regression methods — Linear Regression, Ridge Regression, Lasso Regression, Elastic Net, and K-Nearest Neighbors (KNN) Regressor. These models are measured by a set of error indicators such as MAE, MSE, RMSE, and R². Comparing these models’ performance, I determined the best method in predicting the price of a house. I then look at the feature importances to see which feature of a listing has the greatest effect on the price. Finally, visualizations and prediction results are shown to aid interpretation and validate the conclusions. This report seeks to develop an empirical, data-based overview of US house price dynamics and to provide a simple approach as to what factors contribute most to house prices."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "Code\n#import data\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(lattice)\nlibrary(car)\nlibrary(corrplot)\nlibrary(glmnet)  \nlibrary(randomForest)\nlibrary(rpart)\n \n# Read the data and eliminate the rows containing NA\nset.seed(1234)\n\ndata_raw &lt;- read.csv(\"data/Housing.csv\")\nhead(data_raw)\n\n\n     price area bedrooms bathrooms stories mainroad guestroom basement\n1 13300000 7420        4         2       3      yes        no       no\n2 12250000 8960        4         4       4      yes        no       no\n3 12250000 9960        3         2       2      yes        no      yes\n4 12215000 7500        4         2       2      yes        no      yes\n5 11410000 7420        4         1       2      yes       yes      yes\n6 10850000 7500        3         3       1      yes        no      yes\n  hotwaterheating airconditioning parking prefarea furnishingstatus\n1              no             yes       2      yes        furnished\n2              no             yes       3       no        furnished\n3              no              no       2      yes   semi-furnished\n4              no             yes       3      yes        furnished\n5              no             yes       2       no        furnished\n6              no             yes       2      yes   semi-furnished\n\n\nCode\n# str(data_raw)"
  },
  {
    "objectID": "results.html#implications-for-model-improvement",
    "href": "results.html#implications-for-model-improvement",
    "title": "4  Results",
    "section": "4.1 Implications for Model Improvement",
    "text": "4.1 Implications for Model Improvement\nNonlinear Terms: Incorporate quadratic or spline terms for area or bathrooms to let the model flex at the tails.\nSegmented Models: Fit separate regressions for low-, mid-, and high-priced tiers, or include interaction terms between size and amenity flags.\nAdditional Features: Bring in location quality, age of property, or architectural style to explain why some premium homes exceed the linear model’s ceiling.\nCritical takeaway: While the linear fit is solid overall (R² ≈ 0.69), its tendency to bend predictions away from real extremes uncovers the need for nonlinear adjustments or tiered modeling to faithfully capture the true range of home prices—especially if you’re valuing high-end or entry-level properties.\n\nThis residual plot confirms that, overall, errors are centered around zero with no obvious funnel shape—good news for homoscedasticity. However, two subtler patterns stand out:\n\nClustered Under-Prediction for Mid-Range Homes Between predicted values of roughly –0.5 to +0.5, residuals slightly skew negative (many dots just below zero), meaning the model tends to under-predict true prices for homes in the mid-market segment. It suggests missing mid-tier drivers—perhaps features like neighborhood school ratings or recent renovations not included in our data.\nOccasional Large Positive Outliers There are scattered points with residuals above +1.5 at predicted values around 0 to 1.5. These represent cases where the model severely underestimates some homes’ true value—likely premium properties with rare combinations of features (e.g., a smaller home but in a top school district). These extreme misses hint at nonlinear interactions or omitted variables (like “view” or property age) that our linear formula can’t capture.\nCritical takeaway: While the linear model holds up well in general, its mid-range under-prediction and sporadic large underestimates point to a need for either richer feature sets (e.g., local amenities, year built) or segmented models for different market tiers. Addressing these could tighten residuals and improve trust in pricing recommendations across all property classes."
  }
]