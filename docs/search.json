[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Housing Prices",
    "section": "",
    "text": "1 Introduction\nThe US housing market is affected by multiple factors that result from property physical characteristics, the location related and structural feature. For would-be buyers and sellers, real estate professionals, policy-makers and investors who are seeking data-driven insights, knowing what is behind the price of a house is important. In this experiment, I analyzed what the most important factors are behind the housing price in the Us with a Kaggle dataset (545 observations and 13 features) with what I would call intuitive variables such as area, bedrooms, bathrooms, whether the house has any amenities (e.g., guestroom, basement, air condition) and furnishing status. To perform this analysis I use a set of regression methods — Linear Regression, Ridge Regression, Lasso Regression, Elastic Net, and K-Nearest Neighbors (KNN) Regressor. These models are measured by a set of error indicators such as MAE, MSE, RMSE, and R². Comparing these models’ performance, I determined the best method in predicting the price of a house. I then look at the feature importances to see which feature of a listing has the greatest effect on the price. Finally, visualizations and prediction results are shown to aid interpretation and validate the conclusions. This report seeks to develop an empirical, data-based overview of US house price dynamics and to provide a simple approach as to what factors contribute most to house prices."
  },
  {
    "objectID": "final.html",
    "href": "final.html",
    "title": "2  Housing Price Prediction Analysis",
    "section": "",
    "text": "Code\ngetwd()\n\n\n[1] \"/Users/chensiji/housing\"\n\n\n\n\nCode\n#import data\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(lattice)\nlibrary(car)\nlibrary(corrplot)\nlibrary(glmnet)  \nlibrary(randomForest)\nlibrary(rpart)\n\n# Read the data and eliminate the rows containing NA\nset.seed(1234)\ndata_raw &lt;- read_csv(\"data/Housing.csv\")\nhead(data_raw)\n\n\n# A tibble: 6 × 13\n     price  area bedrooms bathrooms stories mainroad guestroom basement\n     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 13300000  7420        4         2       3 yes      no        no      \n2 12250000  8960        4         4       4 yes      no        no      \n3 12250000  9960        3         2       2 yes      no        yes     \n4 12215000  7500        4         2       2 yes      no        yes     \n5 11410000  7420        4         1       2 yes      yes       yes     \n6 10850000  7500        3         3       1 yes      no        yes     \n# ℹ 5 more variables: hotwaterheating &lt;chr&gt;, airconditioning &lt;chr&gt;,\n#   parking &lt;dbl&gt;, prefarea &lt;chr&gt;, furnishingstatus &lt;chr&gt;\n\n\nCode\nstr(data_raw)\n\n\nspc_tbl_ [545 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ price           : num [1:545] 13300000 12250000 12250000 12215000 11410000 ...\n $ area            : num [1:545] 7420 8960 9960 7500 7420 7500 8580 16200 8100 5750 ...\n $ bedrooms        : num [1:545] 4 4 3 4 4 3 4 5 4 3 ...\n $ bathrooms       : num [1:545] 2 4 2 2 1 3 3 3 1 2 ...\n $ stories         : num [1:545] 3 4 2 2 2 1 4 2 2 4 ...\n $ mainroad        : chr [1:545] \"yes\" \"yes\" \"yes\" \"yes\" ...\n $ guestroom       : chr [1:545] \"no\" \"no\" \"no\" \"no\" ...\n $ basement        : chr [1:545] \"no\" \"no\" \"yes\" \"yes\" ...\n $ hotwaterheating : chr [1:545] \"no\" \"no\" \"no\" \"no\" ...\n $ airconditioning : chr [1:545] \"yes\" \"yes\" \"no\" \"yes\" ...\n $ parking         : num [1:545] 2 3 2 3 2 2 2 0 2 1 ...\n $ prefarea        : chr [1:545] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ furnishingstatus: chr [1:545] \"furnished\" \"furnished\" \"semi-furnished\" \"furnished\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   price = col_double(),\n  ..   area = col_double(),\n  ..   bedrooms = col_double(),\n  ..   bathrooms = col_double(),\n  ..   stories = col_double(),\n  ..   mainroad = col_character(),\n  ..   guestroom = col_character(),\n  ..   basement = col_character(),\n  ..   hotwaterheating = col_character(),\n  ..   airconditioning = col_character(),\n  ..   parking = col_double(),\n  ..   prefarea = col_character(),\n  ..   furnishingstatus = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\nCode\n#Factor transformation\ndata &lt;- data_raw %&gt;%\n  mutate(\n    mainroad          = factor(mainroad,          levels = c(\"yes\", \"no\")),\n    guestroom         = factor(guestroom,         levels = c(\"yes\", \"no\")),\n    basement          = factor(basement,          levels = c(\"yes\", \"no\")),\n    hotwaterheating   = factor(hotwaterheating,   levels = c(\"yes\", \"no\")),\n    airconditioning   = factor(airconditioning,   levels = c(\"yes\", \"no\")),\n    prefarea         = factor(prefarea,          levels = c(\"yes\", \"no\")),\n    furnishingstatus  = factor(furnishingstatus,\n                               levels = c(\"unfurnished\",\"semi-furnished\",\"furnished\"))\n  )\n\n# Dummy variable\ndummy_mod &lt;- dummyVars(~ ., data = data[ , -1], fullRank = TRUE)\ndata_dummies &lt;- predict(dummy_mod, newdata = data) %&gt;% as.data.frame()\n\n# Merge the \"price\" column\ndata2_raw &lt;- bind_cols(price = data$price, data_dummies)\n\nglimpse(data2_raw)\n\n\nRows: 545\nColumns: 14\n$ price                             &lt;dbl&gt; 13300000, 12250000, 12250000, 122150…\n$ area                              &lt;dbl&gt; 7420, 8960, 9960, 7500, 7420, 7500, …\n$ bedrooms                          &lt;dbl&gt; 4, 4, 3, 4, 4, 3, 4, 5, 4, 3, 3, 4, …\n$ bathrooms                         &lt;dbl&gt; 2, 4, 2, 2, 1, 3, 3, 3, 1, 2, 1, 3, …\n$ stories                           &lt;dbl&gt; 3, 4, 2, 2, 2, 1, 4, 2, 2, 4, 2, 2, …\n$ mainroad.no                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ guestroom.no                      &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, …\n$ basement.no                       &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, …\n$ hotwaterheating.no                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ airconditioning.no                &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, …\n$ parking                           &lt;dbl&gt; 2, 3, 2, 3, 2, 2, 2, 0, 2, 1, 2, 2, …\n$ prefarea.no                       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, …\n$ `furnishingstatus.semi-furnished` &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, …\n$ furnishingstatus.furnished        &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, …\n\n\nCode\n#I want to make sure that all columns in data2_raw are of numeric type (dbl), and there are no original factor columns in the column names.\n\n\n\n\nCode\n# Create stratified partition on the price variable\nset.seed(1234)\nidx       &lt;- createDataPartition(data2_raw$price, p = 0.8, list = FALSE)\ntrain_raw &lt;- data2_raw[idx, ]\ntest_raw  &lt;- data2_raw[-idx, ]\n\nscaler    &lt;- preProcess(train_raw, method = c(\"center\", \"scale\"))\n\n# Apply the scaler to both sets\ntrain     &lt;- predict(scaler, train_raw)\ntest      &lt;- predict(scaler, test_raw)\n\n# Confirm scaling: mean≈0, sd≈1\ntrain_summary &lt;- train %&gt;% summarise(across(everything(), list(mean = ~mean(.), sd = ~sd(.))))\nprint(train_summary[, 1:6])  \n\n\n    price_mean price_sd     area_mean area_sd bedrooms_mean bedrooms_sd\n1 1.550257e-15        1 -2.048336e-15       1  8.127378e-16           1\n\n\nCode\n#Standardizing both predictors and the target (price) lets regularized models (Ridge, Lasso) work properly and makes error metrics comparable across features.\n\n\n\n\nCode\n# Linear Regression\nlm_mod &lt;- lm(price ~ ., data = train)\n\n# Ridge Regression (α = 0)\nlibrary(glmnet)\nridge_cv  &lt;- cv.glmnet(\n  x     = as.matrix(train[, -1]),\n  y     = train$price,\n  alpha = 0\n)\nridge_mod &lt;- glmnet(\n  x      = as.matrix(train[, -1]),\n  y      = train$price,\n  alpha  = 0,\n  lambda = ridge_cv$lambda.min\n)\n\n# Lasso Regression (α = 1)\nlasso_cv  &lt;- cv.glmnet(\n  x     = as.matrix(train[, -1]),\n  y     = train$price,\n  alpha = 1\n)\nlasso_mod &lt;- glmnet(\n  x     = as.matrix(train[, -1]),\n  y     = train$price,\n  alpha = 1,\n  lambda= lasso_cv$lambda.min\n)\n\n# Elastic Net (α = 0.5)\nenet_cv  &lt;- cv.glmnet(\n  x     = as.matrix(train[, -1]),\n  y     = train$price,\n  alpha = 0.5\n)\nenet_mod &lt;- glmnet(\n  x      = as.matrix(train[, -1]),\n  y      = train$price,\n  alpha  = 0.5,\n  lambda = enet_cv$lambda.min\n)\n\n# k-Nearest Neighbors\nlibrary(kknn)\nknn_mod &lt;- train.kknn(\n  price ~ .,\n  data  = train,\n  kmax  = 10,\n  kernel= \"optimal\"\n)\n\n\n\n\nCode\n# define evaluation function \nf &lt;- function(true, pred) {\n  mae  &lt;- mean(abs(true - pred))\n  mse  &lt;- mean((true - pred)^2)\n  rmse &lt;- sqrt(mse)\n  r2   &lt;- cor(true, pred)^2\n  c(MAE = mae, MSE = mse, RMSE = rmse, R2 = r2)\n}\n\n# Generate predictions\npred_lm    &lt;- predict(lm_mod,    test)\npred_ridge &lt;- predict(ridge_mod, newx = as.matrix(test[, -1]))\npred_lasso &lt;- predict(lasso_mod, newx = as.matrix(test[, -1]))\npred_enet  &lt;- predict(enet_mod,  newx = as.matrix(test[, -1]))\npred_knn   &lt;- predict(knn_mod,   test)\n\neval_lm    &lt;- f(test$price, pred_lm)\neval_ridge &lt;- f(test$price, pred_ridge)\neval_lasso &lt;- f(test$price, pred_lasso)\neval_enet  &lt;- f(test$price, pred_enet)\neval_knn   &lt;- f(test$price, pred_knn)\n\n# Aggregate results into a data frame\nresults &lt;- rbind(\n  Linear     = eval_lm,\n  Ridge      = eval_ridge,\n  Lasso      = eval_lasso,\n  ElasticNet = eval_enet,\n  KNN        = eval_knn\n) %&gt;% as.data.frame() %&gt;%\n  rownames_to_column(\"Model\")\n\nprint(results)\n\n\n       Model       MAE       MSE      RMSE        R2\n1     Linear 0.4324900 0.3567765 0.5973077 0.6931722\n2      Ridge 0.4349472 0.3595279 0.5996065 0.6942843\n3      Lasso 0.4325785 0.3571445 0.5976156 0.6933268\n4 ElasticNet 0.4326635 0.3572365 0.5976927 0.6933866\n5        KNN 0.4318257 0.4479066 0.6692583 0.6580994\n\n\nCode\nbest_model &lt;- results %&gt;% slice_min(RMSE, n = 1)\ncat(\"Best model by RMSE is:\", best_model$Model, \"\\n\")\n\n\nBest model by RMSE is: Linear \n\n\n\n\nCode\n# IncNodePurity: total reduction in node impurity (variance) when splitting on that feature\n# X.IncMSE: increase in out‐of‐bag MSE if we randomly permute that feature (i.e. how much error jumps when the feature is “hidden”)\nrf_mod &lt;- randomForest(\n  x          = train[, -1],\n  y          = train$price,\n  importance = TRUE\n)\n\nrf_imp &lt;- data.frame(rf_mod$importance) %&gt;%\n  rownames_to_column(\"feature\") %&gt;%\n  pivot_longer(\n    cols      = c(\"X.IncMSE\", \"IncNodePurity\"),\n    names_to  = \"Metric\",\n    values_to = \"Importance\"\n  )\n\nggplot(rf_imp, aes(x = Importance, y = fct_reorder(feature, Importance), color = Metric)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Random Forest Feature Importance\",\n    x     = \"Importance Score\",\n    y     = NULL\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nInsight 1: “Area” Dominance and Its Implications The fact that area towers above every other predictor in both IncNodePurity and permutation‐based (X.IncMSE) importance suggests that sheer square footage is by far the strongest driver of price in this market. That makes intuitive sense—larger homes usually cost more—but it also raises a warning flag: if area is so dominant, our downstream models (especially linear or regularized ones) may end up “coasting” on that one feature and under-utilizing the richer—but weaker—signals in the other covariates. In practice, you might consider:\nNormalizing area relative to neighborhood averages (to capture “premium” size, not just raw square footage).\nAdding interactions (e.g. area × stories) to see if extra space on higher floors actually commands a price premium.\nInsight 2: Categorical Variables Hold Meaningful Signal, but Are We Missing Subtlety? Notice that airconditioning. no (absence of air conditioning) and parking both rank near the top in impurity‐based importance. This tells us buyers penalize homes without a/c and reward those with parking more than, say, extra bedrooms or a semi‐furnished status. However:\nThe red bars for airconditioning. no are smaller than area/bathrooms but still notably large—implying that a/c is almost as critical as having multiple stories or extra bathrooms.\nWe should question whether “no airconditioning” is picking up on a deeper latent factor—perhaps older homes, lower-income areas, or properties without other modern amenities. It might be worthwhile to cluster by age or location to see if absence of a/c is acting as a proxy for something else.\nInsight 3: Consistency Between Importance Metrics The near-perfect alignment between IncNodePurity (red) and X.IncMSE (teal) across nearly all features is reassuring: it means both split‐based and permutation approaches agree on the rank ordering. If they had diverged—say, one metric flagged stories as top-5 but the other didn’t—we’d need to dig in for potential biases (like the impurity measure’s known favoritism toward high-cardinality variables). Here, the agreement suggests our RF importances are stable.\nInsight 4: Lower-Power Features and the Dangers of Overfitting Toward the bottom of the chart you see guestroom. no, furnishingstatus. furnished, and hotwaterheating. no with minimal importance. In a simpler linear model these might still get non-zero coefficients, but the RF is essentially “ignoring” them. This could be because:\nTheir true effect on price is marginal, or\nThe binary splits on these rarely improve homogeneity once the big guns (area, a/c, bathrooms) are in play.\nBefore pruning these variables entirely, though, consider whether they might exhibit nonlinear interactions or only matter in specific sub-markets (e.g. high-end furnished units).\nCritical Takeaway: While area and bathrooms emerge as the “headline” drivers of price, the next tier (air conditioning, parking, stories) reveal the amenity-driven nature of this housing market. Yet, the very dominance of area suggests we should refine our feature engineering—perhaps by normalizing by lot size or introducing spatial/contextual variables—so that the models learn a richer, more nuanced story beyond “bigger = pricier.”\n\n\nCode\n#Feature importance\n# Fit a simple decision tree\ntree_mod &lt;- rpart(price ~ ., data = train, method = \"anova\")\n\n# Extract and normalize importances\ndt_imp &lt;- tibble(\n  feature    = names(tree_mod$variable.importance),\n  Importance = tree_mod$variable.importance\n) %&gt;%\n  mutate(Importance = 100 * Importance / sum(Importance),\n         Model      = \"Decision Tree\")\n\nrf_imp2 &lt;- data.frame(rf_mod$importance) %&gt;%\n  rownames_to_column(\"feature\") %&gt;%\n  transmute(\n    feature,\n    Importance = 100 * IncNodePurity / sum(IncNodePurity),\n    Model      = \"Random Forest\"\n  )\n\n# Top 5 each\ntop_dt &lt;- dt_imp %&gt;% slice_max(Importance, n = 5)\ntop_rf &lt;- rf_imp2 %&gt;% slice_max(Importance, n = 5)\n\ncmp_imp &lt;- bind_rows(top_dt, top_rf)\n\nggplot(cmp_imp, aes(x = Importance, y = fct_reorder(feature, Importance), fill = Model)) +\n  geom_col(position = position_dodge(width = 0.8)) +\n  labs(\n    title = \"Top 5 Feature Importances: Tree vs. Forest\",\n    x     = \"Relative Importance (%)\",\n    y     = NULL\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nInsight 1: Area and Bathrooms Consistently Reign Supreme Both single decision tree and the averaged random forest agree that area is the top predictor—no surprises there, but it’s striking how much more weight the tree gives area (over 40%) compared to the forest (around 32%). Similarly, bathrooms show up as the clear second driver, though again the tree inflates its importance relative to the forest. This suggests that while a single tree may over-rely on these two variables to split early and gain purity, the forest’s ensemble dampens that effect slightly by distributing splits across more varied features.\nInsight 2: Stories vs. Air Conditioning—A Role Reversal In the decision tree, stories (number of floors) edge out airconditioning. no (lack of A/C) as the third most important, but in the forest they essentially swap places. That flip hints at instability: a lone tree might find “two-story” splits very useful in its particular bootstrap sample, whereas the full forest, averaging hundreds of trees, finds the absence of air conditioning to be a slightly more reliable signal overall. It tells us to be cautious about treating any single tree’s ranking as definitive.\nInsight 3: Parking’s Steady Climb Notice parking climbs from last among the top-five in the single tree to solidly in fourth place in the forest. This upward adjustment reflects the forest’s ability to uncover subtler patterns—in this case, that having designated parking repeatedly reduces error across many trees, even if it wasn’t the very first split in the standalone tree. It reinforces the idea that parking is a non-negligible amenity premium in this market.\nInsight 4: Model Bias vs. Ensemble Stability Overall, the tree shows a more “peaked” distribution—putting over 40% weight on area, 20% on bathrooms—while the forest spreads importance more evenly (~32%, ~18%, ~9%, ~9%, ~8%). This contrast illustrates a classic bias–variance trade-off: a single tree can latch heavily onto a few strong predictors (biasing its view of importance), whereas the forest ensemble smooths out those biases, yielding a more conservative but robust ranking. For feature engineering, it means we should trust the forest’s ordering more when deciding which variables to focus on or transform further.\n\n\nCode\n# Partial Dependence Plots for Top RF Features\n\nlibrary(pdp)\nlibrary(purrr)\n\n# Pick the top 6 features by IncNodePurity\ntop_features &lt;- names(sort(rf_mod$importance[,\"IncNodePurity\"], decreasing = TRUE))[1:6]\n\n# Compute PDPs\npdp_df &lt;- map_dfr(top_features, function(var) {\n  pd &lt;- partial(rf_mod, pred.var = var, train = as.data.frame(train))\n  pd %&gt;%\n    rename(feature_value = 1) %&gt;%\n    mutate(feature = var)\n})\n\nggplot(pdp_df, aes(x = feature_value, y = yhat)) +\n  geom_line() +\n  facet_wrap(~ feature, scales = \"free_x\") +\n  labs(\n    title = \"Partial Dependence Plots (Random Forest)\",\n    x     = \"Feature Value\",\n    y     = \"Predicted Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nInsight 1: Diminishing Returns in “Area” Beyond a Threshold The partial dependence for area isn’t purely linear—there’s a noticeable knee around the 1–2 standard-deviation mark. Below that, each incremental increase in size has a modest effect, but once you clear that threshold, the predicted price jumps sharply before tailing off again at the very high end. This suggests a nonlinear premium on “mid-sized” homes (perhaps family-sized properties in desirable ranges) and diminishing value beyond ultra-large mansions. For product strategy or valuation, it means simply adding square footage isn’t equally valuable at every level—there’s a “sweet spot” where buyers pay disproportionately more.\nInsight 2: Bathroom Count Plateaus After Two We see a steep rise in predicted price when moving from zero to two bathrooms, but beyond two, the curve flattens out. In other words, going from one to two baths is a huge upgrade in market perception, but adding a third or fourth only marginally lifts price. That plateau signals diminishing returns on over-equipping smaller properties—investing in that third bathroom may not pay off unless the home is already very large or high-end.\nInsight 3: Stories Ignite an Accelerating Premium The plot for stories (floors) slopes gently at first, then curves upward more steeply for three-story homes. This accelerating pattern hints that while a second floor adds normal living space, multi-level designs (e.g. three stories) carry an architectural or prestige premium. It could reflect penthouse or attic conversions commanding extra value. From a development perspective, encouraging vertical expansion might yield larger returns past a certain point.\nInsight 4: Amenity Binary Effects\nAirconditioning. no: The PDP line drops from about +0.2 to –0.1 when “no A/C,” revealing a net 0.3-sd penalty—buyers strongly disfavor homes without climate control.\nParking: There’s a steady lift with each additional spot, but the slope diminishes after two spaces. This suggests first- and second- spots are most valuable (perhaps for families), whereas adding a third or fourth car space yields less marginal benefit.\nCritical Takeaway: These PDPs uncover thresholds and plateaus that simple feature‐coefficient plots can’t capture. They show:\nNonlinear sweet spots (area, stories) where investment returns spike.\nDiminishing returns on certain upgrades (bathrooms beyond two, extra parking).\nBinary penalties on missing amenities (no A/C).\nArmed with these insights, you can tailor recommendations—focus on hitting those sweet-spot thresholds in home size and amenities, rather than blanket “bigger is better” or “more is better” strategies.\n\n\nCode\n# Final Model Diagnostics & Conclusion\n\nlibrary(broom)\nlibrary(ggplot2)\n\nlm_coefs &lt;- tidy(lm_mod) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(signif = p.value &lt; 0.05)\n\n# Coefficient Plot with ±1 SE\nggplot(lm_coefs, aes(x = estimate, y = fct_reorder(term, estimate), color = signif)) +\n  geom_point(size = 3) +\n  geom_errorbarh(aes(xmin = estimate - std.error, xmax = estimate + std.error), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"grey50\") +\n  scale_color_manual(values = c(\"TRUE\" = \"#4E79A7\", \"FALSE\" = \"#E15759\")) +\n  labs(\n    title = \"Linear Regression Coefficients (±1 SE)\",\n    x     = \"Estimate\",\n    y     = \"Feature\",\n    color = \"p &lt; 0.05\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nCode\n# Actual vs. Predicted\npred_lm &lt;- predict(lm_mod, test)\ndf_pred &lt;- data.frame(\n  Actual    = test$price,\n  Predicted = pred_lm\n)\n\nggplot(df_pred, aes(x = Actual, y = Predicted)) +\n  geom_point(alpha = 0.7, color = \"darkgreen\") +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Actual vs. Predicted Prices (Linear Regression)\",\n    x     = \"Actual (standardized)\",\n    y     = \"Predicted (standardized)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nCode\n# Residuals vs. Predicted\nresid_lm &lt;- df_pred$Actual - df_pred$Predicted\n\nggplot(df_pred, aes(x = Predicted, y = resid_lm)) +\n  geom_point(alpha = 0.6, color = \"darkred\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Residuals vs. Predicted (Linear Regression)\",\n    x     = \"Predicted\",\n    y     = \"Residual\"\n  ) +\n  theme_minimal()\n\n\n\n\n\nCode\n# Conclusion \ncat(\n  \"Conclusion:\\n\",\n  \"- The Linear Regression model achieved RMSE =\", round(results[\"Linear\",\"RMSE\"], 4),\n  \"and R² =\", round(results[\"Linear\",\"R2\"], 4), \"on the test set.\\n\",\n  \"- Key positive drivers are:\", \n    paste(lm_coefs %&gt;% filter(estimate &gt; 0 & signif) %&gt;% pull(term), collapse = \", \"), \".\\n\",\n  \"- Key negative drivers are:\", \n    paste(lm_coefs %&gt;% filter(estimate &lt; 0 & signif) %&gt;% pull(term), collapse = \", \"), \".\\n\",\n  \"- Residual plots show no severe heteroscedasticity or nonlinear patterns, supporting model validity.\\n\"\n)\n\n\nConclusion:\n - The Linear Regression model achieved RMSE = NA and R² = NA on the test set.\n - Key positive drivers are: area, bathrooms, stories, parking, `furnishingstatus.semi-furnished`, furnishingstatus.furnished .\n - Key negative drivers are: mainroad.no, basement.no, hotwaterheating.no, airconditioning.no, prefarea.no .\n - Residual plots show no severe heteroscedasticity or nonlinear patterns, supporting model validity.\n\n\n\nHere, the linear model’s coefficients echo—and slightly nuance—the random forest findings, but they also raise fresh questions:\n\nBathrooms vs. Area Trade-off Although both bathrooms and area carry large, significant positive effects, you’ll notice the error bars on bathrooms are narrower. That tells us each extra bathroom is a more precisely estimated driver of price than raw square footage—perhaps because bathroom counts are less noisy to measure than lot size. In practice, this suggests an investor might get a more reliable ROI by adding or renovating bathrooms rather than simply expanding square footage.\nStories Carry a Premium, but with More Uncertainty The stories coefficient is also strongly positive, yet its confidence interval is wider than that of bathrooms or area. This hints at greater variability in how extra floors translate to value—maybe because some two-story layouts add livable space efficiently, while others (e.g. split levels) confuse buyers. It’s a reminder to look beneath the “more floors = higher price” rule and examine design quality or neighborhood norms.\nAmenities Show Clear Penalties Every one of our binary “no amenity” flags—airconditioning.no, prefarea.no, basement.no, hotwaterheating.no, mainroad.no, guestroom.no—pulls price downward. Airconditioning.no sits at roughly –0.22, a hefty penalty, confirming climate control is almost non-negotiable. Less obvious is the modest negative for mainroad.no (living off a main road)—which may reflect both peace-and-quiet preferences and potential access concerns. These small-magnitude, significant effects underline the importance of even subtle locational and amenity cues in linear pricing.\nNon-Significant Variables Warn Against Overfitting Notice bedrooms and guestroom.no aren’t significant here, unlike in our random forest. The linear model effectively shrinks their estimates to near-zero when other features are in play, suggesting that once you control for bathrooms and area, simply adding a bedroom or designating a guest room doesn’t reliably increase price. That guards us against piling on variables that look meaningful in isolation but vanish in a multivariate context.\nDeeper takeaway: This coefficient plot shows not just which features matter, but how reliably and by how much. It cautions us that even under a “linear” assumption, amenity flags can be as powerful as raw size metrics—and that some features (like extra floors) carry high potential value but also higher uncertainty. In your report, you might recommend focusing capital on the most precisely estimated upgrades (bathrooms, A/C, parking), while treating more variable features (stories, lot size expansions) with targeted market research or pilot projects before large investments.\n\nThis “Actual vs. Predicted” plot does more than confirm a decent overall fit (points roughly hugging the 45° line); it reveals systematic quirks at the extremes and suggests where the linear model could be improved:\n\nUnderprediction at the High End Notice the rightmost points—homes with very large standardized actual values (around +4)—are predicted lower (around +3). The model is “shrinking” the extreme prices back toward the mean, a classic sign of regression toward the mean. It tells us our linear model fails to capture the full upside value of truly premium properties, perhaps because of missing high-end amenities or nonlinear size effects.\nMild Overprediction at the Low End At the opposite tail (actual ≈ –1.2), the predictions cluster just above (around –1.0). The model again regresses extremes inward, slightly overestimating the cheapest homes’ prices. This could reflect floor effects (there’s a minimum viable market price) or that our “no amenity” flags don’t fully explain very low-priced segments.\nWider Scatter in the Midrange Between standardized actual values of –0.5 to +2.0, the cloud of points shows moderate vertical dispersion—predictions vary by ±0.5 sd around the line. That midrange scatter indicates heteroscedasticity: the model is less precise for “typical” homes than for extreme cases, perhaps because there are more competing factors (location, condition, style) in that middle market.\nImplications for Model Improvement\nNonlinear Terms: Incorporate quadratic or spline terms for area or bathrooms to let the model flex at the tails.\nSegmented Models: Fit separate regressions for low-, mid-, and high-priced tiers, or include interaction terms between size and amenity flags.\nAdditional Features: Bring in location quality, age of property, or architectural style to explain why some premium homes exceed the linear model’s ceiling.\nCritical takeaway: While the linear fit is solid overall (R² ≈ 0.69), its tendency to bend predictions away from real extremes uncovers the need for nonlinear adjustments or tiered modeling to faithfully capture the true range of home prices—especially if you’re valuing high-end or entry-level properties.\n\nThis residual plot confirms that, overall, errors are centered around zero with no obvious funnel shape—good news for homoscedasticity. However, two subtler patterns stand out:\n\nClustered Under-Prediction for Mid-Range Homes Between predicted values of roughly –0.5 to +0.5, residuals slightly skew negative (many dots just below zero), meaning the model tends to under-predict true prices for homes in the mid-market segment. It suggests missing mid-tier drivers—perhaps features like neighborhood school ratings or recent renovations not included in our data.\nOccasional Large Positive Outliers There are scattered points with residuals above +1.5 at predicted values around 0 to 1.5. These represent cases where the model severely underestimates some homes’ true value—likely premium properties with rare combinations of features (e.g., a smaller home but in a top school district). These extreme misses hint at nonlinear interactions or omitted variables (like “view” or property age) that our linear formula can’t capture.\nCritical takeaway: While the linear model holds up well in general, its mid-range under-prediction and sporadic large underestimates point to a need for either richer feature sets (e.g., local amenities, year built) or segmented models for different market tiers. Addressing these could tighten residuals and improve trust in pricing recommendations across all property classes."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "3  Data",
    "section": "",
    "text": "Code\n#import data\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(lattice)\nlibrary(car)\nlibrary(corrplot)\nlibrary(glmnet)  \nlibrary(randomForest)\nlibrary(rpart)\n \n# Read the data and eliminate the rows containing NA\nset.seed(1234)\n\ndata_raw &lt;- read.csv(\"data/Housing.csv\")\nhead(data_raw)\n\n\n     price area bedrooms bathrooms stories mainroad guestroom basement\n1 13300000 7420        4         2       3      yes        no       no\n2 12250000 8960        4         4       4      yes        no       no\n3 12250000 9960        3         2       2      yes        no      yes\n4 12215000 7500        4         2       2      yes        no      yes\n5 11410000 7420        4         1       2      yes       yes      yes\n6 10850000 7500        3         3       1      yes        no      yes\n  hotwaterheating airconditioning parking prefarea furnishingstatus\n1              no             yes       2      yes        furnished\n2              no             yes       3       no        furnished\n3              no              no       2      yes   semi-furnished\n4              no             yes       3      yes        furnished\n5              no             yes       2       no        furnished\n6              no             yes       2      yes   semi-furnished\n\n\nCode\n# str(data_raw)"
  },
  {
    "objectID": "results.html#implications-for-model-improvement",
    "href": "results.html#implications-for-model-improvement",
    "title": "5  Results",
    "section": "5.1 Implications for Model Improvement",
    "text": "5.1 Implications for Model Improvement\nNonlinear Terms: Incorporate quadratic or spline terms for area or bathrooms to let the model flex at the tails.\nSegmented Models: Fit separate regressions for low-, mid-, and high-priced tiers, or include interaction terms between size and amenity flags.\nAdditional Features: Bring in location quality, age of property, or architectural style to explain why some premium homes exceed the linear model’s ceiling.\nCritical takeaway: While the linear fit is solid overall (R² ≈ 0.69), its tendency to bend predictions away from real extremes uncovers the need for nonlinear adjustments or tiered modeling to faithfully capture the true range of home prices—especially if you’re valuing high-end or entry-level properties.\n\nThis residual plot confirms that, overall, errors are centered around zero with no obvious funnel shape—good news for homoscedasticity. However, two subtler patterns stand out:\n\nClustered Under-Prediction for Mid-Range Homes Between predicted values of roughly –0.5 to +0.5, residuals slightly skew negative (many dots just below zero), meaning the model tends to under-predict true prices for homes in the mid-market segment. It suggests missing mid-tier drivers—perhaps features like neighborhood school ratings or recent renovations not included in our data.\nOccasional Large Positive Outliers There are scattered points with residuals above +1.5 at predicted values around 0 to 1.5. These represent cases where the model severely underestimates some homes’ true value—likely premium properties with rare combinations of features (e.g., a smaller home but in a top school district). These extreme misses hint at nonlinear interactions or omitted variables (like “view” or property age) that our linear formula can’t capture.\nCritical takeaway: While the linear model holds up well in general, its mid-range under-prediction and sporadic large underestimates point to a need for either richer feature sets (e.g., local amenities, year built) or segmented models for different market tiers. Addressing these could tighten residuals and improve trust in pricing recommendations across all property classes."
  },
  {
    "objectID": "objective.html",
    "href": "objective.html",
    "title": "2  objective",
    "section": "",
    "text": "The main goal of this project is to understand and measure the determinants affecting housing prices in the US. The objective of the task is twofold: (Using multiple regression methods based on model fitness index.) Identification of Key Features: Identify the most impactful housing attributes – for example: area, number of rooms, utilities, or the geographical location the house is situated in. Predicting housing prices: The task is to develop an accurate model that can estimate the price of a house based on its features. My goal with this project is to cull patterns to guide real estate related decision-making and to offer practical insight into the housing market."
  },
  {
    "objectID": "data overview.html#feature-description",
    "href": "data overview.html#feature-description",
    "title": "3  Data Overview",
    "section": "3.1 Feature Description",
    "text": "3.1 Feature Description\nFeature Type Description price Numeric Selling price of the house (target variable) area Numeric Size of the house in square feet bedrooms Numeric Number of bedrooms bathrooms Numeric Number of bathrooms stories Numeric Number of stories (floors) mainroad Categorical Whether the house is connected to the main road (Yes/No) guestroom Categorical Presence of a guest room (Yes/No) basement Categorical Presence of a basement (Yes/No) hotwaterheating Categorical Availability of hot water heating (Yes/No) airconditioning Categorical Availability of air conditioning (Yes/No) parking Numeric Number of parking spaces prefarea Categorical Whether the house is in a preferred area (Yes/No) furnishingstatus Categorical Furnishing status (e.g., furnished, semi-furnished, unfurnished)"
  },
  {
    "objectID": "data overview.html#initial-observations",
    "href": "data overview.html#initial-observations",
    "title": "3  Data Overview",
    "section": "3.2 Initial Observations",
    "text": "3.2 Initial Observations\nThe data, including binary, and multi-class which are pre-processed to be suitable for modeling. No missing values were detected in the dataset, and hence analysis should be straightforward. The variable area would seem to be most obviously related to cost, as it is known that larger properties are more expensive."
  }
]