# Results & Analysis
```{r}
#import data
library(tidyverse)
library(caret)
library(lattice)
library(car)
library(corrplot)
library(glmnet)  
library(randomForest)
library(rpart)

# Read the data and eliminate the rows containing NA
set.seed(1234)
data_raw <- read_csv("~/housing/data/Housing.csv")


#Factor transformation
data <- data_raw %>%
  mutate(
    mainroad          = factor(mainroad,          levels = c("yes", "no")),
    guestroom         = factor(guestroom,         levels = c("yes", "no")),
    basement          = factor(basement,          levels = c("yes", "no")),
    hotwaterheating   = factor(hotwaterheating,   levels = c("yes", "no")),
    airconditioning   = factor(airconditioning,   levels = c("yes", "no")),
    prefarea         = factor(prefarea,          levels = c("yes", "no")),
    furnishingstatus  = factor(furnishingstatus,
                               levels = c("unfurnished","semi-furnished","furnished"))
  )

# Dummy variable
dummy_mod <- dummyVars(~ ., data = data[ , -1], fullRank = TRUE)
data_dummies <- predict(dummy_mod, newdata = data) %>% as.data.frame()

# Merge the "price" column
data2_raw <- bind_cols(price = data$price, data_dummies)



```
```{r}
# Create stratified partition on the price variable
set.seed(1234)
idx       <- createDataPartition(data2_raw$price, p = 0.8, list = FALSE)
train_raw <- data2_raw[idx, ]
test_raw  <- data2_raw[-idx, ]

scaler    <- preProcess(train_raw, method = c("center", "scale"))

# Apply the scaler to both sets
train     <- predict(scaler, train_raw)
test      <- predict(scaler, test_raw)

# Confirm scaling: mean≈0, sd≈1
train_summary <- train %>% summarise(across(everything(), list(mean = ~mean(.), sd = ~sd(.))))
 

#Standardizing both predictors and the target (price) lets regularized models (Ridge, Lasso) work properly and makes error metrics comparable across features.


# Linear Regression
lm_mod <- lm(price ~ ., data = train)

# Ridge Regression (α = 0)
library(glmnet)
ridge_cv  <- cv.glmnet(
  x     = as.matrix(train[, -1]),
  y     = train$price,
  alpha = 0
)
ridge_mod <- glmnet(
  x      = as.matrix(train[, -1]),
  y      = train$price,
  alpha  = 0,
  lambda = ridge_cv$lambda.min
)

# Lasso Regression (α = 1)
lasso_cv  <- cv.glmnet(
  x     = as.matrix(train[, -1]),
  y     = train$price,
  alpha = 1
)
lasso_mod <- glmnet(
  x     = as.matrix(train[, -1]),
  y     = train$price,
  alpha = 1,
  lambda= lasso_cv$lambda.min
)

# Elastic Net (α = 0.5)
enet_cv  <- cv.glmnet(
  x     = as.matrix(train[, -1]),
  y     = train$price,
  alpha = 0.5
)
enet_mod <- glmnet(
  x      = as.matrix(train[, -1]),
  y      = train$price,
  alpha  = 0.5,
  lambda = enet_cv$lambda.min
)

# k-Nearest Neighbors
library(kknn)
knn_mod <- train.kknn(
  price ~ .,
  data  = train,
  kmax  = 10,
  kernel= "optimal"
)


# define evaluation function 
f <- function(true, pred) {
  mae  <- mean(abs(true - pred))
  mse  <- mean((true - pred)^2)
  rmse <- sqrt(mse)
  r2   <- cor(true, pred)^2
  c(MAE = mae, MSE = mse, RMSE = rmse, R2 = r2)
}

# Generate predictions
pred_lm    <- predict(lm_mod,    test)
pred_ridge <- predict(ridge_mod, newx = as.matrix(test[, -1]))
pred_lasso <- predict(lasso_mod, newx = as.matrix(test[, -1]))
pred_enet  <- predict(enet_mod,  newx = as.matrix(test[, -1]))
pred_knn   <- predict(knn_mod,   test)

eval_lm    <- f(test$price, pred_lm)
eval_ridge <- f(test$price, pred_ridge)
eval_lasso <- f(test$price, pred_lasso)
eval_enet  <- f(test$price, pred_enet)
eval_knn   <- f(test$price, pred_knn)

# Aggregate results into a data frame
results <- rbind(
  Linear     = eval_lm,
  Ridge      = eval_ridge,
  Lasso      = eval_lasso,
  ElasticNet = eval_enet,
  KNN        = eval_knn
) %>% as.data.frame() %>%
  rownames_to_column("Model")


best_model <- results %>% slice_min(RMSE, n = 1)


```
This section highlights the performance of each regression model and most importantly interprets key results from both numerical evaluation and feature analysis.

## Feature Importance Across Models

After crossing all models, the top-ranked features were remarkably consistent:

| **Feature**          | **Importance Signal (Ridge/Lasso)** | **Interpretation**                                                   |
|----------------------|--------------------------------------|-----------------------------------------------------------------------|
| area                 | Very High                           | Strong, direct effect on price—larger homes sell for more.            |
| airconditioning      | High                                | Adds perceived comfort and luxury, justifying a price premium.        |
| parking              | High                                | Reflects urban convenience—more parking = higher demand.              |
| prefarea             | Moderate–High                       | Captures neighborhood desirability; a strong location proxy.          |
| bathrooms            | Moderate                            | Additional bathrooms correlate with size and utility.                 |
| furnishingstatus     | Low–Moderate (varies by model)      | Has subtle impact; may overlap with socioeconomic signals.            |

## Heuristic Reasoning and Feature Relationships

**Overlap:**  
Some features seem to measure overlapping concepts: `area`, `stories`, and `bathrooms` all relate to house size and livability. This suggests multicollinearity, and regularisation (Lasso/Ridge) will favor `area` as the most influential feature.  
Similarly, `prefarea` and `furnishingstatus` might each serve as a proxy for socioeconomic desirability (furnishing might simply echo neighborhood-based norms or buyer wealth), but neither clearly dominates on its own.

**Binary Characteristics and Step-Value Effects:**  
Some categorical features (e.g., `airconditioning`, `basement`, `guestroom`) exhibited step-function patterns in PDPs:

- **Air conditioning** was on average associated with a large, positive shift in predicted price compared to homes without A/C.
- **Basement and Guestroom** also had noticeable effects, but their utility appears to be highly context-dependent—e.g., basements may be more valued in suburban regions than urban cores or flood-prone zones.

## Visualization Highlights
To better understand how feature influence predictions, these are the findings which were found:
```{r}
# IncNodePurity: total reduction in node impurity (variance) when splitting on that feature
# X.IncMSE: increase in out‐of‐bag MSE if we randomly permute that feature (i.e. how much error jumps when the feature is “hidden”)
library(randomForest)
library(dplyr)
library(tidyr)
library(forcats)
library(ggplot2)
library(tibble)    # for rownames_to_column()

# 1) Fit RF
rf_mod <- randomForest(
  x          = train[, -1],
  y          = train$price,
  importance = TRUE
)

# 2) Tidy importance
rf_imp <- rf_mod$importance %>% 
  as.data.frame() %>% 
  rownames_to_column("feature") %>% 
  pivot_longer(
    cols      = c("%IncMSE", "IncNodePurity"),    # use the actual column names
    names_to  = "Metric",
    values_to = "Importance"
  )

# 3) Plot with facets
ggplot(rf_imp, aes(
    x    = Importance,
    y    = fct_reorder(feature, Importance),
    fill = Metric
  )) +
  geom_col() +
  facet_wrap(~ Metric, scales = "free_x") +
  labs(
    title = "Random Forest Feature Importance",
    x     = "Importance Score",
    y     = NULL
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    strip.text      = element_text(face = "bold")
  )
```
```{r}
# Partial Dependence Plots for Top RF Features

library(pdp)
library(purrr)

# Pick the top 6 features by IncNodePurity
top_features <- names(sort(rf_mod$importance[,"IncNodePurity"], decreasing = TRUE))[1:6]

# Compute PDPs
pdp_df <- map_dfr(top_features, function(var) {
  pd <- partial(rf_mod, pred.var = var, train = as.data.frame(train))
  pd %>%
    rename(feature_value = 1) %>%
    mutate(feature = var)
})

ggplot(pdp_df, aes(x = feature_value, y = yhat)) +
  geom_line() +
  facet_wrap(~ feature, scales = "free_x") +
  labs(
    title = "Partial Dependence Plots (Random Forest)",
    x     = "Feature Value",
    y     = "Predicted Price"
  ) +
  theme_minimal()

```
As mentioned above, `area` has a major character in both IncNodePurity and permutation-based importance, which means larger homes require higher prices. However as mentioned before the model relies too heavily on area and this will bring the risk of ignoring subtler-still meaningful signals in the data. Houses growing from under average to about half size increases its predicted price dramatically, especially in the area around 1-2 standard deviations above the mean. But beyond this “sweet spot,” price appreciation trails off, implying that ultra-large homes bring little in the way of incremental value unless they're in the luxury bracket.

Bathroom count is subject to similar logic: going from 1 to 2 bathrooms gives a large increase in home value, which is perhaps a more practical requirement for families. But a third or fourth bathroom does little unless it is combined with other upscale features such as extra stories or square footage.

Stories (number of floors) also present a different kind of threshold: The curve of added value steepens after two stories, suggesting that multi-storied architecture has, for some reason, a cachet or a design appeal that isn’t entirely a matter of more space. This could be for penthouses, lofts, or split-level plans.

**Improvement and Refinements:**

- Adjusting `area` for the neighborhood average (so it can fill it with “premium” square footage, not just square footage).
- Including interactions (e.g. area × stories) to test whether extra space on higher floors sells at a price premium.

**Categorical Amenities Add Tangible Value**

Noticing that `airconditioning.no` (lack of air conditioning) and parking are among the most impure‐important. This tells us buyers penalize homes without a/c and reward those with parking more than, say, extra bedrooms or a semi‐furnished status.

The bar on the chart for `airconditioning.no` are smaller values than `area`/`bathrooms` but still quite significant, which suggests a/c is almost just as important as having more than one story or bathroom.

This brought a question that “Might `airconditioning.no` be picking up on a deeper underlying factor such as older homes, lower-income neighborhoods or properties lacking other modern-day amenities?”  It might be interesting to stratify by age or location to check for absence in a/c is essentially serving as a proxy for something else.

**Weak Features and Overfitting Risk**

Towards the bottom of the chart shows that `guestroom.no`, `furnishingstatus.furnished` and `hotwaterheating.no` with minimal importance. In a more simple linear model, these may still receive non zero coefficients, but the Random Forest basically completely “ignores” them. The reason for this is because: their real impact on price is negligible though, or these binary splits rarely do anything to help homogeneity once the big kings (`area`, `a/c`, `bathrooms`) are on the board. Before dropping these variables completely, however, it is considerable to think about whether they may have nonlinear interactions, or only be important at certain sub-markets (eg. high-end furnished units).

**Critical Takeaway**

`area` and `bathrooms` are left right where sellers would expect to see them, as the “headline” determinants of price point, while the next tier (air conditioning, parking, stories) illustrate an amenity driven market of housing. However, the overwhelming importance of area screams that it is necessary to have a better feature engineering for it - for example, maybe normalizing by lot size or adding something non-linear based on location/area, and forcing the model to learn some more complex story than **“bigger = more expensive.”**

```{r}
#Feature importance
# Fit a simple decision tree
tree_mod <- rpart(price ~ ., data = train, method = "anova")

# Extract and normalize importances
dt_imp <- tibble(
  feature    = names(tree_mod$variable.importance),
  Importance = tree_mod$variable.importance
) %>%
  mutate(Importance = 100 * Importance / sum(Importance),
         Model      = "Decision Tree")

rf_imp2 <- data.frame(rf_mod$importance) %>%
  rownames_to_column("feature") %>%
  transmute(
    feature,
    Importance = 100 * IncNodePurity / sum(IncNodePurity),
    Model      = "Random Forest"
  )

# Top 5 each
top_dt <- dt_imp %>% slice_max(Importance, n = 5)
top_rf <- rf_imp2 %>% slice_max(Importance, n = 5)

cmp_imp <- bind_rows(top_dt, top_rf)

ggplot(cmp_imp, aes(x = Importance, y = fct_reorder(feature, Importance), fill = Model)) +
  geom_col(position = position_dodge(width = 0.8)) +
  labs(
    title = "Top 5 Feature Importances: Tree vs. Forest",
    x     = "Relative Importance (%)",
    y     = NULL
  ) +
  theme_minimal() +
  theme(legend.position = "top")

```
**Parking Gains Credibility in the Forest**

Expecting that high-tech transportation companies would be raiding the ranks of New York’s taxi drivers for drivers. Observing the parking goes from a last of the top-five in the single tree forest to a solid fourth place in the forest. This increase shows that by looking at many trees, the forest is able to find subtle patterns. It contributes to the thesis that parking is a non-negligible amenity premium in this market.

**Model Bias & Ensemble Stability**

All in all, the tree has a stronger distinguishing signal put into a more “peaked” form: It puts 40+ % of its weight on area, 20% on bathrooms, while the forest is spreading the importance more evenly(~32%,~18%,~9% x5, ~8% x5). This contrast exemplifies the classic bias–variance trade-off: one tree will easily latch onto a few strong predictors (biasing its view of importance), while the forest ensemble will iron out those biases, leaving a more conservative but more robust ranking. When doing feature engineering, it means we should have more faith in the forest ordering to decide on which variables to pay more attention to or transform more.

```{r}
# Final Model Diagnostics & Conclusion

library(broom)
library(ggplot2)

lm_coefs <- tidy(lm_mod) %>%
  filter(term != "(Intercept)") %>%
  mutate(signif = p.value < 0.05)

# Coefficient Plot with ±1 SE
ggplot(lm_coefs, aes(x = estimate, y = fct_reorder(term, estimate), color = signif)) +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = estimate - std.error, xmax = estimate + std.error), height = 0.2) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "grey50") +
  scale_color_manual(values = c("TRUE" = "#4E79A7", "FALSE" = "#E15759")) +
  labs(
    title = "Linear Regression Coefficients (±1 SE)",
    x     = "Estimate",
    y     = "Feature",
    color = "p < 0.05"
  ) +
  theme_minimal()

# Actual vs. Predicted
pred_lm <- predict(lm_mod, test)
df_pred <- data.frame(
  Actual    = test$price,
  Predicted = pred_lm
)

ggplot(df_pred, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.7, color = "darkgreen") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    title = "Actual vs. Predicted Prices (Linear Regression)",
    x     = "Actual (standardized)",
    y     = "Predicted (standardized)"
  ) +
  theme_minimal()

# Residuals vs. Predicted
resid_lm <- df_pred$Actual - df_pred$Predicted

ggplot(df_pred, aes(x = Predicted, y = resid_lm)) +
  geom_point(alpha = 0.6, color = "darkred") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Residuals vs. Predicted (Linear Regression)",
    x     = "Predicted",
    y     = "Residual"
  ) +
  theme_minimal()

# Conclusion 
cat(
  "Conclusion:\n",
  "- The Linear Regression model achieved RMSE =", round(results["Linear","RMSE"], 4),
  "and R² =", round(results["Linear","R2"], 4), "on the test set.\n",
  "- Key positive drivers are:", 
    paste(lm_coefs %>% filter(estimate > 0 & signif) %>% pull(term), collapse = ", "), ".\n",
  "- Key negative drivers are:", 
    paste(lm_coefs %>% filter(estimate < 0 & signif) %>% pull(term), collapse = ", "), ".\n",
  "- Residual plots show no severe heteroscedasticity or nonlinear patterns, supporting model validity.\n"
)

```

**Actual vs. Predicted Values**

Although the actual vs. predicted plot indicates that the models fits the data well overall (most points falling near the diagonal), closer examination reveals a systematic bias:

- **High-End Underprediction:**
When high-priced houses are predicted to be lower priced-(basically regression to the mean), perhaps because of the lack of features in the data available to the model in luxury-tier properties or some nonlinear function of sizes.

- **Low-End Overprediction:**
Properties with low cost are underestimated, which seems to reflect a floor effect, or suboptimal representation of low-end restrictions in the data.

- **Mid-Range Scatter:**
Higher scatter in the middle of the market (between –0.5 and +2 standard units) indicates higher model uncertainty, possibly because of unmodelled variation such as renovation quality, micro-location, or architectural style.

**Model Limitations and Paths Forward**
Although the linear model is an effective and transparent model, some of its drawbacks are centeredness-bias and inability to describe extreme values, for which one may consider the following improvements:

- **Nonlinear Terms:** Using quadratic/spline transformations for area or bathrooms resolves the shape of the response curve.
- **Tiered Modeling:** Split the data into low/mid/high priced homes and estimate different models (or including interaction terms)
- **Feature Enrichment:** Incorporate geographic, school quality, or architectural characteristics that probably explain variation not captured in core variables.

**Residual Analysis Confirms Homoscedasticity with Pockets of Concern**

The residuals vs. fitted plot suggests a generally homoscedastic model (errors are spread equally across the value 0), but it indicates two problems:

- **Mild Underprediction for Mid-Range Properties:** 
Residuals centrally are slanted a bit towards underprediction for predicted values ranging between –0.5 and +0.5, especially for the systemically undervalued “average” houses— due perhaps to lacking some middle-of-the-market features (or maybe location quirks).
- **Extreme Underprediction for Certain Homes:**  
We have some observations with residuals > +1.5, which could be extreme cases where the model doesn't capture premium value for whatever reason (missing features, such as school district, view, or year built).
- **Takeaway:** 
Though the residuals look sensible overall, targeted feature engineering (probably based on neighborhood or home age) may fix some of these larger misses and rein in mid-tier predictions.
