# Methodology
This part will introduce the modeling process which is used to analyze the housing prices, which includes preprocessing steps, model selection, evaluation metrics, and feature importance analysis. 

```{r}
#import data
library(tidyverse)
library(caret)
library(lattice)
library(car)
library(corrplot)
library(glmnet)  
library(randomForest)
library(rpart)

# Read the data and eliminate the rows containing NA
set.seed(1234)
data_raw <- read_csv("~/housing/data/Housing.csv")
head(data_raw)
str(data_raw)

```
## Data Preprocessing
In order to do the regression modeling for the dataset, several preprocessing steps need to be applied:


```{r}
#Factor transformation
data <- data_raw %>%
  mutate(
    mainroad          = factor(mainroad,          levels = c("yes", "no")),
    guestroom         = factor(guestroom,         levels = c("yes", "no")),
    basement          = factor(basement,          levels = c("yes", "no")),
    hotwaterheating   = factor(hotwaterheating,   levels = c("yes", "no")),
    airconditioning   = factor(airconditioning,   levels = c("yes", "no")),
    prefarea         = factor(prefarea,          levels = c("yes", "no")),
    furnishingstatus  = factor(furnishingstatus,
                               levels = c("unfurnished","semi-furnished","furnished"))
  )

# Dummy variable
dummy_mod <- dummyVars(~ ., data = data[ , -1], fullRank = TRUE)
data_dummies <- predict(dummy_mod, newdata = data) %>% as.data.frame()

# Merge the "price" column
data2_raw <- bind_cols(price = data$price, data_dummies)

glimpse(data2_raw)
#I want to make sure that all columns in data2_raw are of numeric type (dbl), and there are no original factor columns in the column names.

```
```{r}
# Create stratified partition on the price variable
set.seed(1234)
idx       <- createDataPartition(data2_raw$price, p = 0.8, list = FALSE)
train_raw <- data2_raw[idx, ]
test_raw  <- data2_raw[-idx, ]

scaler    <- preProcess(train_raw, method = c("center", "scale"))

# Apply the scaler to both sets
train     <- predict(scaler, train_raw)
test      <- predict(scaler, test_raw)

# Confirm scaling: mean≈0, sd≈1
train_summary <- train %>% summarise(across(everything(), list(mean = ~mean(.), sd = ~sd(.))))
print(train_summary[, 1:6])  

#Standardizing both predictors and the target (price) lets regularized models (Ridge, Lasso) work properly and makes error metrics comparable across features.


```


- **Handling Categorical Features:**
The binary categorical variables (mainroad, guestroom, basement, hotwaterheating, airconditioning, prefarea), were converted to numeric by assigning value 0 to “No” and 1 to “Yes”. The furnishingstatus variable, which has more than two levels, was one-hot encoded to prevent imposing ordinal dependencies where there are none.
- **Normalization:**
For feature scale to sensitive models (Ridge, Lasso, Elastic Net, and KNN), numeric features were all standardized by z-score scaling. This prevents features with high numerical ranges (like area) from unduly influencing the model.
- **Train-Test Split:**
The data were randomly partitioned into a training set comprised of 80% of the participants, and a testing set was obtained with the remaining 20% to assess the generalization ability of the model. The division was randomized but the same for all models to facilitate proper comparison.


Five different models were implemented to predict house prices:

- **Linear Regression**  
- **Ridge Regression**  
- **Lasso Regression**  
- **Elastic Net Regression**  
- **K-Nearest Neighbors (KNN) Regression**

---

## Model Evaluation Metrics

To assess and compare model performance, the following evaluation metrics were used:

- **Mean Absolute Error (MAE)**  
- **Mean Squared Error (MSE)**  
- **Root Mean Squared Error (RMSE)**  
- **R² Score (Coefficient of Determination)**

These metrics provide complementary views of error magnitude and explanatory power.

---

## Feature Importance

Feature importance was assessed using multiple techniques tailored to different model types:

- **Coefficient-Based Importance**  
  For linear models (Linear, Ridge, Lasso, and Elastic Net), the magnitude of standardized coefficients was used to rank feature importance.

- **Permutation-Based Importance**  
  For non-linear or non-parametric models like KNN, permutation importance was used to measure how randomly shuffling each feature affects model error.

- **Partial Dependence Plots (PDPs)**  
  PDPs were used to visualize the marginal effect of each feature on predicted price while holding other variables constant.

- **SHAP-like Reasoning**  
  Though SHAP values were not computed directly, similar interpretive logic was applied to understand individual feature contributions based on model behavior and prediction shifts.